{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: LLMs en CrewAI\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Los Large Language Models (LLMs) son el núcleo de inteligencia detrás de los agentes de CrewAI. Permiten a los agentes entender contexto, tomar decisiones y generar respuestas similares a las humanas.\n",
    "\n",
    "### ¿Qué son los LLMs?\n",
    "\n",
    "- **Definición**: Sistemas de IA entrenados en vastas cantidades de datos de texto\n",
    "- **Función**: Proporcionan inteligencia a los agentes de CrewAI\n",
    "- **Capacidades**: Entender y generar texto similar al humano\n",
    "\n",
    "### Conceptos Clave\n",
    "\n",
    "1. **Ventana de Contexto**: Determina cuánto texto puede procesar un LLM a la vez\n",
    "   - Ventanas más grandes (ej: 128K tokens) = más contexto pero más caro y lento\n",
    "   - Ventanas más pequeñas = más rápido pero menos contexto\n",
    "\n",
    "2. **Temperatura** (0.0 a 1.0): Controla la aleatoriedad de las respuestas\n",
    "   - Valores bajos (ej: 0.2) = respuestas más enfocadas y deterministas\n",
    "   - Valores altos (ej: 0.8) = más creatividad y variabilidad\n",
    "\n",
    "3. **Proveedores**: Cada proveedor (OpenAI, Anthropic, Google) ofrece diferentes modelos con capacidades, precios y características variadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de LLMs\n",
    "\n",
    "Hay tres formas principales de configurar LLMs en CrewAI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuración por Variables de Entorno\n",
    "\n",
    "La forma más simple de comenzar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de archivo .env\n",
    "MODEL=model-id  # ej: gpt-4o, gemini-2.0-flash, claude-3-sonnet-...\n",
    "\n",
    "# Asegúrate de configurar también tus API keys\n",
    "OPENAI_API_KEY=sk-...\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "GEMINI_API_KEY=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuración por YAML\n",
    "\n",
    "Excelente para control de versiones y colaboración en equipo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents.yaml\n",
    "researcher:\n",
    "  role: Research Specialist\n",
    "  goal: Conduct comprehensive research and analysis\n",
    "  backstory: A dedicated research professional with years of experience\n",
    "  verbose: true\n",
    "  llm: provider/model-id  # ej: openai/gpt-4o, google/gemini-2.0-flash, anthropic/claude..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuración Directa en Python\n",
    "\n",
    "Para máxima flexibilidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# Configuración básica\n",
    "llm = LLM(model=\"model-id-here\")  # gpt-4o, gemini-2.0-flash, anthropic/claude...\n",
    "\n",
    "# Configuración avanzada con parámetros detallados\n",
    "llm = LLM(\n",
    "    model=\"model-id-here\",  # gpt-4o, gemini-2.0-flash, anthropic/claude...\n",
    "    temperature=0.7,        # Mayor para salidas más creativas\n",
    "    timeout=120,            # Segundos para esperar respuesta\n",
    "    max_tokens=4000,        # Longitud máxima de respuesta\n",
    "    top_p=0.9,             # Parámetro de muestreo nucleus\n",
    "    frequency_penalty=0.1,  # Reduce repetición\n",
    "    presence_penalty=0.1,   # Fomenta diversidad de temas\n",
    "    response_format={\"type\": \"json\"},  # Para salidas estructuradas\n",
    "    seed=42                 # Para resultados reproducibles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación de Parámetros\n",
    "\n",
    "- **temperature**: Controla aleatoriedad (0.0-1.0)\n",
    "- **timeout**: Tiempo máximo de espera para respuesta\n",
    "- **max_tokens**: Limita longitud de respuesta\n",
    "- **top_p**: Alternativa a temperature para muestreo\n",
    "- **frequency_penalty**: Reduce repetición de palabras\n",
    "- **presence_penalty**: Fomenta nuevos temas\n",
    "- **response_format**: Especifica estructura de salida\n",
    "- **seed**: Asegura salidas consistentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proveedores de LLMs\n",
    "\n",
    "CrewAI soporta múltiples proveedores de LLMs. Aquí tienes ejemplos de configuración:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "Uno de los principales proveedores con amplia gama de modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables de entorno\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPENAI_API_BASE=\n",
    "OPENAI_ORGANIZATION=\n",
    "\n",
    "# Uso en CrewAI\n",
    "from crewai import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"openai/gpt-4\",  # llamar modelo por provider/model_name\n",
    "    temperature=0.8,\n",
    "    max_tokens=150,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    "    stop=[\"END\"],\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de OpenAI\n",
    "\n",
    "| Modelo | Ventana de Contexto | Mejor Para |\n",
    "|--------|---------------------|------------|\n",
    "| GPT-4 | 8,192 tokens | Tareas de alta precisión, razonamiento complejo |\n",
    "| GPT-4 Turbo | 128,000 tokens | Contenido largo, análisis de documentos |\n",
    "| GPT-4o & GPT-4o-mini | 128,000 tokens | Procesamiento de contexto grande coste-efectivo |\n",
    "| o3-mini | 200,000 tokens | Razonamiento rápido, razonamiento complejo |\n",
    "| o1-mini | 128,000 tokens | Razonamiento rápido, razonamiento complejo |\n",
    "| o1-preview | 128,000 tokens | Razonamiento rápido, razonamiento complejo |\n",
    "| o1 | 200,000 tokens | Razonamiento rápido, razonamiento complejo |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Llama\n",
    "\n",
    "API de Meta que proporciona acceso a la familia de modelos Llama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables de entorno\n",
    "LLAMA_API_KEY=LLM|your_api_key_here\n",
    "\n",
    "# Uso en CrewAI\n",
    "from crewai import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8\",\n",
    "    temperature=0.8,\n",
    "    stop=[\"END\"],\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de Meta Llama\n",
    "\n",
    "| Model ID | Input context length | Output context length | Input Modalities | Output Modalities |\n",
    "|----------|---------------------|----------------------|------------------|-------------------|\n",
    "| `meta_llama/Llama-4-Scout-17B-16E-Instruct-FP8` | 128k | 4028 | Text, Image | Text |\n",
    "| `meta_llama/Llama-4-Maverick-17B-128E-Instruct-FP8` | 128k | 4028 | Text, Image | Text |\n",
    "| `meta_llama/Llama-3.3-70B-Instruct` | 128k | 4028 | Text | Text |\n",
    "| `meta_llama/Llama-3.3-8B-Instruct` | 128k | 4028 | Text | Text |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic (Claude)\n",
    "\n",
    "Modelos avanzados de Anthropic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables de entorno\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "ANTHROPIC_API_BASE=\n",
    "\n",
    "# Uso en CrewAI\n",
    "llm = LLM(\n",
    "    model=\"anthropic/claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google (Gemini)\n",
    "\n",
    "Modelos de Google AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables de entorno\n",
    "GEMINI_API_KEY=\n",
    "\n",
    "# Uso en CrewAI\n",
    "llm = LLM(\n",
    "    model=\"google/gemini-2.0-flash\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros Proveedores\n",
    "\n",
    "CrewAI también soporta:\n",
    "\n",
    "- **Cerebras**: Velocidades de inferencia rápidas, precios competitivos\n",
    "- **OpenRouter**: Acceso a múltiples modelos a través de una API\n",
    "- **Nebius**: Gran colección de modelos de código abierto\n",
    "- **Groq**: Velocidades ultra-rápidas\n",
    "- **Ollama**: Modelos locales\n",
    "- **Together AI**: Modelos de código abierto\n",
    "- **Cohere**: Modelos especializados\n",
    "- **Fireworks AI**: Modelos de alto rendimiento\n",
    "- **DeepSeek**: Modelos avanzados\n",
    "- **Qwen**: Familia de modelos de Alibaba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuestas en Streaming\n",
    "\n",
    "CrewAI soporta respuestas en streaming desde LLMs, permitiendo recibir y procesar salidas en tiempo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# Crear un LLM con streaming habilitado\n",
    "llm = LLM(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    stream=True  # Habilitar streaming\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventos de Streaming\n",
    "\n",
    "CrewAI emite eventos para cada chunk recibido durante el streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.utilities.events import LLMStreamChunkEvent\n",
    "from crewai.utilities.events.base_event_listener import BaseEventListener\n",
    "\n",
    "class MyCustomListener(BaseEventListener):\n",
    "    def setup_listeners(self, crewai_event_bus):\n",
    "        @crewai_event_bus.on(LLMStreamChunkEvent)\n",
    "        def on_llm_stream_chunk(self, event: LLMStreamChunkEvent):\n",
    "            # Procesar cada chunk conforme llega\n",
    "            print(f\"Received chunk: {event.chunk}\")\n",
    "\n",
    "my_listener = MyCustomListener()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado por Agente y Tarea\n",
    "\n",
    "Todos los eventos de LLM en CrewAI incluyen información de agente y tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM, Agent, Task, Crew\n",
    "from crewai.utilities.events import LLMStreamChunkEvent\n",
    "from crewai.utilities.events.base_event_listener import BaseEventListener\n",
    "\n",
    "class MyCustomListener(BaseEventListener):\n",
    "    def setup_listeners(self, crewai_event_bus):\n",
    "        @crewai_event_bus.on(LLMStreamChunkEvent)\n",
    "        def on_llm_stream_chunk(source, event):\n",
    "            if researcher.id == event.agent_id:\n",
    "                print(\"\\n==============\\n Got event:\", event, \"\\n==============\\n\")\n",
    "\n",
    "my_listener = MyCustomListener()\n",
    "\n",
    "llm = LLM(model=\"gpt-4o-mini\", temperature=0, stream=True)\n",
    "\n",
    "researcher = Agent(\n",
    "    role=\"About User\",\n",
    "    goal=\"You know everything about the user.\",\n",
    "    backstory=\"\"\"You are a master at understanding people and their preferences.\"\"\",\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "search = Task(\n",
    "    description=\"Answer the following questions about the user: {question}\",\n",
    "    expected_output=\"An answer to the question.\",\n",
    "    agent=researcher,\n",
    ")\n",
    "\n",
    "crew = Crew(agents=[researcher], tasks=[search])\n",
    "\n",
    "result = crew.kickoff(\n",
    "    inputs={\"question\": \"...\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llamadas LLM Estructuradas\n",
    "\n",
    "CrewAI soporta respuestas estructuradas de llamadas LLM permitiendo definir un `response_format` usando un modelo Pydantic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    breed: str\n",
    "\n",
    "llm = LLM(model=\"gpt-4o\", response_format=Dog)\n",
    "\n",
    "response = llm.call(\n",
    "    \"Analyze the following messages and return the name, age, and breed. \"\n",
    "    \"Meet Kona! She is 3 years old and is a black german shepherd.\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "# Output:\n",
    "# Dog(name='Kona', age=3, breed='black german shepherd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características Avanzadas y Optimización\n",
    "\n",
    "### Gestión Inteligente de Contexto\n",
    "\n",
    "CrewAI incluye características de gestión de contexto inteligentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "\n",
    "# CrewAI maneja automáticamente:\n",
    "# 1. Conteo y seguimiento de tokens\n",
    "# 2. Resumen de contenido cuando es necesario\n",
    "# 3. División de tareas para contextos grandes\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"gpt-4\",\n",
    "    max_tokens=4000,  # Limitar longitud de respuesta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejores Prácticas para Gestión de Contexto\n",
    "\n",
    "1. **Elegir modelos con ventanas de contexto apropiadas**\n",
    "2. **Pre-procesar entradas largas cuando sea posible**\n",
    "3. **Usar chunking para documentos grandes**\n",
    "4. **Monitorear uso de tokens para optimizar costos**\n",
    "\n",
    "### Elegir la Ventana de Contexto Correcta\n",
    "\n",
    "- **Tareas pequeñas** (hasta 4K tokens): Modelos estándar\n",
    "- **Tareas medianas** (entre 4K-32K): Modelos mejorados\n",
    "- **Tareas grandes** (más de 32K): Modelos de contexto grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar modelo con configuraciones apropiadas\n",
    "llm = LLM(\n",
    "    model=\"openai/gpt-4-turbo-preview\",\n",
    "    temperature=0.7,    # Ajustar basado en la tarea\n",
    "    max_tokens=4096,    # Establecer basado en necesidades de salida\n",
    "    timeout=300         # Timeout más largo para tareas complejas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de Costos\n",
    "\n",
    "1. **Monitorear uso de tokens**\n",
    "2. **Implementar rate limiting**\n",
    "3. **Usar caching cuando sea posible**\n",
    "4. **Establecer límites max_tokens apropiados**\n",
    "\n",
    "### Configuración de Temperatura\n",
    "\n",
    "- **Temperatura baja** (0.1 a 0.3) para respuestas factuales\n",
    "- **Temperatura alta** (0.7 a 0.9) para tareas creativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros Opcionales\n",
    "\n",
    "CrewAI usa internamente LiteLLM, permitiendo omitir parámetros adicionales que no necesites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "o3_llm = LLM(\n",
    "    model=\"o3\",\n",
    "    drop_params=True,\n",
    "    additional_drop_params=[\"stop\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas Comunes y Soluciones\n",
    "\n",
    "### Problemas de Autenticación\n",
    "\n",
    "La mayoría de problemas de autenticación se resuelven verificando formato de API key y nombres de variables de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n",
    "# Anthropic\n",
    "ANTHROPIC_API_KEY=sk-ant-..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nombres de Modelos Incorrectos\n",
    "\n",
    "Siempre incluir el prefijo del proveedor en nombres de modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcto\n",
    "llm = LLM(model=\"openai/gpt-4\")\n",
    "\n",
    "# Incorrecto\n",
    "llm = LLM(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas de Contexto\n",
    "\n",
    "Usar modelos de contexto grande para tareas extensas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de contexto grande\n",
    "llm = LLM(model=\"openai/gpt-4o\")  # 128K tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este tutorial hemos cubierto:\n",
    "\n",
    "1. **Conceptos básicos de LLMs** en CrewAI\n",
    "2. **Tres métodos de configuración**: variables de entorno, YAML y Python directo\n",
    "3. **Múltiples proveedores**: OpenAI, Meta Llama, Anthropic, Google, y otros\n",
    "4. **Características avanzadas**: streaming, respuestas estructuradas, gestión de contexto\n",
    "5. **Optimización**: mejores prácticas para costos y rendimiento\n",
    "6. **Solución de problemas**: problemas comunes y sus soluciones\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "- Experimenta con diferentes modelos y configuraciones\n",
    "- Implementa streaming para aplicaciones en tiempo real\n",
    "- Optimiza tus configuraciones basándote en tus necesidades específicas\n",
    "- Monitorea el uso de tokens para optimizar costos\n",
    "\n",
    "Recuerda revisar regularmente tu configuración de LLM y ajustarla según sea necesario para optimizar costos y rendimiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
